{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p = \"Software_5.json.gz\"\n",
    "\n",
    "df = pd.read_json(p, lines=True, compression=\"gzip\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43bcc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop image and style column because there are too many missing values\n",
    "df = df.drop(columns=['image','style'], errors='ignore')\n",
    "\n",
    "# The missing values in our vote variable will be filled with 0\n",
    "df['vote'] = pd.to_numeric(df['vote'].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "print(df.columns.tolist())\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7222aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Graph to show how many >5 reviews there are vs. how many <5 reviews there are\n",
    "y = (df['vote'] >= 5).astype(int)\n",
    "\n",
    "y.value_counts().sort_index().plot(kind='bar')\n",
    "plt.xticks([0,1], ['<5 Reviews','≥5 Reviews']); plt.ylabel('count'); plt.title('Class balance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot to show relationship if number of characters in t heir review affect the review \n",
    "df['text_len'] = (df['summary'].fillna('') + ' ' + df['reviewText'].fillna('')).str.len()\n",
    "plt.scatter(df['text_len'], df['vote'], s=5, alpha=0.3)\n",
    "plt.yscale('log'); plt.xlabel(' text length'); plt.ylabel('helpful votes (log scale)')\n",
    "plt.title('Text length vs helpful votes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177edfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Import and robust DATA_PATH\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    brier_score_loss,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "\n",
    "cwd = Path.cwd()\n",
    "print(\"Current working directory:\", cwd)\n",
    "\n",
    "matches = list(cwd.rglob(\"Software_5.json.gz\"))\n",
    "print(\"Matches found:\", matches)\n",
    "\n",
    "if not matches:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find 'Software_5.json.gz' under {cwd}. \"\n",
    "        \"Check that the file is in the repo and the name matches exactly.\"\n",
    "    )\n",
    "\n",
    "DATA_PATH = matches[0]  # first match\n",
    "print(\"Using DATA_PATH:\", DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load data and basic cleaning\n",
    "#Data Source: The University of California, San Diego, cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews\n",
    "\n",
    "# Amazon review files are usually json lines with gzip compression\n",
    "df = pd.read_json(DATA_PATH, lines=True, compression=\"gzip\")\n",
    "\n",
    "# Keep only the columns we care about\n",
    "cols_keep = [\n",
    "    \"overall\", \"verified\", \"reviewTime\", \"reviewerID\", \"asin\",\n",
    "    \"reviewerName\", \"reviewText\", \"summary\",\n",
    "    \"unixReviewTime\", \"vote\"\n",
    "]\n",
    "df = df[cols_keep].copy()\n",
    "\n",
    "#Clean 'vote' and define target\n",
    "# vote is stored as strings like \"3\", \"1,234\", or may be missing\n",
    "df[\"vote\"] = df[\"vote\"].fillna(\"0\").astype(str).str.replace(\",\", \"\", regex=False)\n",
    "df[\"vote\"] = pd.to_numeric(df[\"vote\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Binary target: helpful if > 5 votes\n",
    "df[\"helpful\"] = (df[\"vote\"] > 5).astype(int)\n",
    "\n",
    "# ---- Clean text fields ----\n",
    "df[\"reviewText\"] = df[\"reviewText\"].fillna(\"\")\n",
    "df[\"summary\"] = df[\"summary\"].fillna(\"\")\n",
    "\n",
    "# ---- Basic time handling ----\n",
    "# unixReviewTime is seconds since epoch; ensure numeric and sort\n",
    "df[\"unixReviewTime\"] = pd.to_numeric(df[\"unixReviewTime\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"unixReviewTime\"])\n",
    "df = df.sort_values(\"unixReviewTime\").reset_index(drop=True)\n",
    "\n",
    "print(df.head())\n",
    "print(df[\"helpful\"].value_counts(normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603468a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature engineering\n",
    "\n",
    "\n",
    "#Simple text-based scalar features\n",
    "df[\"review_len_chars\"] = df[\"reviewText\"].str.len()\n",
    "df[\"review_len_words\"] = df[\"reviewText\"].str.split().str.len()\n",
    "df[\"summary_len_words\"] = df[\"summary\"].str.split().str.len()\n",
    "\n",
    "df[\"has_exclamation\"] = df[\"reviewText\"].str.contains(\"!\", regex=False).astype(int)\n",
    "df[\"has_question\"] = df[\"reviewText\"].str.contains(r\"\\?\", regex=True).astype(int)\n",
    "\n",
    "#Verified -> numeric\n",
    "df[\"verified_int\"] = df[\"verified\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Chronological Train / Val / Test split\n",
    "\n",
    "y = df[\"helpful\"].values\n",
    "X = df.copy()   # we’ll keep all feature columns in X for now\n",
    "\n",
    "n = len(df)\n",
    "train_end = int(0.70 * n)\n",
    "val_end = int(0.85 * n)\n",
    "\n",
    "X_train = X.iloc[:train_end].copy()\n",
    "y_train = y[:train_end]\n",
    "\n",
    "X_val = X.iloc[train_end:val_end].copy()\n",
    "y_val = y[train_end:val_end]\n",
    "\n",
    "X_test = X.iloc[val_end:].copy()\n",
    "y_test = y[val_end:]\n",
    "\n",
    "print(\"Train/Val/Test sizes:\", len(X_train), len(X_val), len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b69b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Reviewer & product aggregate features\n",
    "\n",
    "def build_agg_features(train_df):\n",
    "    # Reviewer-level aggregates on train\n",
    "    reviewer_stats = train_df.groupby(\"reviewerID\").agg(\n",
    "        reviewer_total_reviews=(\"overall\", \"size\"),\n",
    "        reviewer_mean_rating=(\"overall\", \"mean\"),\n",
    "        reviewer_mean_vote=(\"vote\", \"mean\"),\n",
    "    )\n",
    "\n",
    "    # Product-level aggregates on train\n",
    "    product_stats = train_df.groupby(\"asin\").agg(\n",
    "        product_total_reviews=(\"overall\", \"size\"),\n",
    "        product_mean_rating=(\"overall\", \"mean\"),\n",
    "        product_mean_vote=(\"vote\", \"mean\"),\n",
    "    )\n",
    "\n",
    "    return reviewer_stats, product_stats\n",
    "\n",
    "\n",
    "def merge_agg_features(X_part, reviewer_stats, product_stats):\n",
    "    out = X_part.copy()\n",
    "\n",
    "    out = out.merge(\n",
    "        reviewer_stats,\n",
    "        how=\"left\",\n",
    "        left_on=\"reviewerID\",\n",
    "        right_index=True,\n",
    "    )\n",
    "    out = out.merge(\n",
    "        product_stats,\n",
    "        how=\"left\",\n",
    "        left_on=\"asin\",\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    # Fill missing aggregates with global means from training stats\n",
    "    for col in [\n",
    "        \"reviewer_total_reviews\", \"reviewer_mean_rating\", \"reviewer_mean_vote\",\n",
    "        \"product_total_reviews\", \"product_mean_rating\", \"product_mean_vote\",\n",
    "    ]:\n",
    "        if col in out:\n",
    "            out[col] = out[col].fillna(reviewer_stats[col].mean() if col in reviewer_stats.columns\n",
    "                                       else product_stats[col].mean())\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "reviewer_stats, product_stats = build_agg_features(X_train)\n",
    "\n",
    "X_train = merge_agg_features(X_train, reviewer_stats, product_stats)\n",
    "X_val   = merge_agg_features(X_val, reviewer_stats, product_stats)\n",
    "X_test  = merge_agg_features(X_test, reviewer_stats, product_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ColumnTransformer: numeric + text\n",
    "\n",
    "numeric_features = [\n",
    "    \"overall\",\n",
    "    \"verified_int\",\n",
    "    \"review_len_chars\",\n",
    "    \"review_len_words\",\n",
    "    \"summary_len_words\",\n",
    "    \"has_exclamation\",\n",
    "    \"has_question\",\n",
    "    \"reviewer_total_reviews\",\n",
    "    \"reviewer_mean_rating\",\n",
    "    \"reviewer_mean_vote\",\n",
    "    \"product_total_reviews\",\n",
    "    \"product_mean_rating\",\n",
    "    \"product_mean_vote\",\n",
    "]\n",
    "\n",
    "text_feature = \"summary\"\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "text_transformer = Pipeline(\n",
    "    steps=[\n",
    "        # Limit features so it fits comfortably in memory\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),\n",
    "        # PCA-like dimensionality reduction on sparse TF-IDF\n",
    "        (\"svd\", TruncatedSVD(n_components=100, random_state=RANDOM_STATE)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"txt\", text_transformer, text_feature),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ac636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Helper functions: training + evaluation\n",
    "\n",
    "\n",
    "def make_timeseries_cv(n_splits=5):\n",
    "    return TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "\n",
    "def evaluate_model(model, X, y, threshold=0.5, label=\"\"):\n",
    "    \"\"\"Compute a set of metrics for a trained model.\"\"\"\n",
    "    y_proba = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    roc_auc = roc_auc_score(y, y_proba)\n",
    "    pr_auc = average_precision_score(y, y_proba)\n",
    "    brier = brier_score_loss(y, y_proba)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "    print(f\"\\n=== {label} (threshold = {threshold:.3f}) ===\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"PR AUC:\", pr_auc)\n",
    "    print(\"Brier score:\", brier)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"brier\": brier,\n",
    "        \"cm\": cm,\n",
    "        \"threshold\": threshold,\n",
    "    }\n",
    "\n",
    "\n",
    "def choose_threshold_from_val(model, X_val, y_val):\n",
    "    \"\"\"Pick threshold that maximizes F1 on the validation set.\"\"\"\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    thresholds = np.linspace(0.1, 0.9, 33)  # 0.1, 0.12...,0.9\n",
    "    best_f1 = -1\n",
    "    best_thr = 0.5\n",
    "    for thr in thresholds:\n",
    "        y_pred = (y_proba >= thr).astype(int)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(\n",
    "            y_val, y_pred, average=\"binary\", zero_division=0\n",
    "        )\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = thr\n",
    "    return best_thr, best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d254bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Model 1: K-Nearest Neighbors\n",
    "\n",
    "knn_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"clf\", KNeighborsClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tscv = make_timeseries_cv(n_splits=4)\n",
    "\n",
    "knn_param_grid = {\n",
    "    \"clf__n_neighbors\": [3, 5, 11],\n",
    "    \"clf__weights\": [\"uniform\", \"distance\"],\n",
    "}\n",
    "\n",
    "knn_grid = GridSearchCV(\n",
    "    knn_pipe,\n",
    "    param_grid=knn_param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"average_precision\",  # PR-AUC\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "knn_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best KNN params:\", knn_grid.best_params_)\n",
    "knn_best = knn_grid.best_estimator_\n",
    "\n",
    "# Choose threshold on validation set\n",
    "knn_thr, knn_val_f1 = choose_threshold_from_val(knn_best, X_val, y_val)\n",
    "print(\"KNN best validation F1:\", knn_val_f1, \"at threshold\", knn_thr)\n",
    "\n",
    "# Evaluate on val and test\n",
    "knn_val_metrics = evaluate_model(knn_best, X_val, y_val, threshold=knn_thr, label=\"KNN - Validation\")\n",
    "knn_test_metrics = evaluate_model(knn_best, X_test, y_test, threshold=knn_thr, label=\"KNN - Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Model 2: Logistic Regression (class_weight='balanced')\n",
    "\n",
    "log_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "            solver=\"lbfgs\",\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "log_param_grid = {\n",
    "    \"clf__C\": [0.01, 0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "log_grid = GridSearchCV(\n",
    "    log_pipe,\n",
    "    param_grid=log_param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"average_precision\",  # PR-AUC\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "log_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Logistic Regression params:\", log_grid.best_params_)\n",
    "log_best = log_grid.best_estimator_\n",
    "\n",
    "# Choose threshold on validation set\n",
    "log_thr, log_val_f1 = choose_threshold_from_val(log_best, X_val, y_val)\n",
    "print(\"LogReg best validation F1:\", log_val_f1, \"at threshold\", log_thr)\n",
    "\n",
    "# Evaluate on val and test\n",
    "log_val_metrics = evaluate_model(log_best, X_val, y_val, threshold=log_thr, label=\"Logistic Regression - Validation\")\n",
    "log_test_metrics = evaluate_model(log_best, X_test, y_test, threshold=log_thr, label=\"Logistic Regression - Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Model 3: Small Neural Network (MLPClassifier)\n",
    "\n",
    "mlp_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"clf\", MLPClassifier(\n",
    "            hidden_layer_sizes=(64,),\n",
    "            activation=\"relu\",\n",
    "            alpha=1e-3,\n",
    "            batch_size=256,\n",
    "            learning_rate=\"adaptive\",\n",
    "            max_iter=100,          # keep small for stability; can raise\n",
    "            early_stopping=True,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mlp_param_grid = {\n",
    "    \"clf__hidden_layer_sizes\": [(32,), (64,), (64, 32)],\n",
    "    \"clf__alpha\": [1e-4, 1e-3, 1e-2],\n",
    "}\n",
    "\n",
    "mlp_grid = GridSearchCV(\n",
    "    mlp_pipe,\n",
    "    param_grid=mlp_param_grid,\n",
    "    cv=tscv,\n",
    "    scoring=\"average_precision\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "mlp_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best MLP params:\", mlp_grid.best_params_)\n",
    "mlp_best = mlp_grid.best_estimator_\n",
    "\n",
    "# Choose threshold on validation set\n",
    "mlp_thr, mlp_val_f1 = choose_threshold_from_val(mlp_best, X_val, y_val)\n",
    "print(\"MLP best validation F1:\", mlp_val_f1, \"at threshold\", mlp_thr)\n",
    "\n",
    "# Evaluate on val and test\n",
    "mlp_val_metrics = evaluate_model(mlp_best, X_val, y_val, threshold=mlp_thr, label=\"MLP - Validation\")\n",
    "mlp_test_metrics = evaluate_model(mlp_best, X_test, y_test, threshold=mlp_thr, label=\"MLP - Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. PR and ROC curves for the chosen model\n",
    "\n",
    "def plot_curves(model, X, y, split_label=\"Test\"):\n",
    "    y_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Precision-Recall\n",
    "    precision, recall, _ = precision_recall_curve(y, y_proba)\n",
    "    pr_auc = average_precision_score(y, y_proba)\n",
    "\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y, y_proba)\n",
    "    roc_auc = roc_auc_score(y, y_proba)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(recall, precision)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"{split_label} Precision-Recall Curve (AP = {pr_auc:.3f})\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{split_label} ROC Curve (AUC = {roc_auc:.3f})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example: curves for best logistic model on test\n",
    "plot_curves(log_best, X_test, y_test, split_label=\"Test - Logistic Regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61053e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Model\": [\"KNN\", \"Logistic Regression\", \"MLP Neural Network\"],\n",
    "    \"Accuracy (Test)\": [0.930244664, 0.872982821, 0.809994974],\n",
    "    \"Precision\": [0.123955, 0.129707, 0.079096],\n",
    "    \"Recall\": [0.164176, 0.462865, 0.419791],\n",
    "    \"F1\": [0.140216, 0.202634, 0.133017],\n",
    "    \"ROC-AUC\": [0.744366, 0.804396, 0.744356],\n",
    "    \"PR-AUC\": [0.0670065, 0.126437, 0.099509],\n",
    "    \"Threshold\": [0.35, 0.475, 0.25]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(data)\n",
    "df_results\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
