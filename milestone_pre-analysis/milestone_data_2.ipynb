{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75bed370",
   "metadata": {},
   "source": [
    "Methodology Overview - Rachel Kang\n",
    "\n",
    "The objective of this project is to predict whether an Amazon product will review will receive more than 5 helpful votes at the time the review is posted. Our approach is to use a binary classifciation model using variables that are known at the time of the posting, so that our model is reflective of a realistic situation. \n",
    "\n",
    "In the previous milestone, we had already cleaned and processed the data so that we could use it for analysis. We had removed variables such as \"image\" and \"style\" because they had too many missing variables which did not serve a good purpose for our model. All missing values in the \"vote\" column were turned into 0, which reflects a review that had received no helpful reviews. \n",
    "\n",
    "To define the target variable, we had a binary threshold of greater than 5 votes. Reviews that got more than 5 votes were deemed as \"helpful\" while those that received less than 5 votes were deemed as \"not helpful\". Text features were processed into numerical representations which were better suited for modeling. Numerical variables  such as star rating and review length were scaled as needed. \n",
    "\n",
    "Class imbalance were a major characteristic of the dataset, with the many of the votes receiving less than 5 votes. To address this, we want to use class weighting to help mitigate the bias within the model from being too tipped one way. In addition, preprocessing and feature coding will be implemented to ensure that input variables capture relevant information wihout introducing any bias or data leaks which would vastly improve the model's ability to generalize positive or negative reviews. We also plan to evaluate the relative importance of different features and compare how various preprocessing stretegies influece the results. By analyzing text and the numeric parts together, we aim to identify which parts will contribute the most to the model in the next portion of the project. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466912fd",
   "metadata": {},
   "source": [
    "Model Overview and Justification - Morgan Simmons\n",
    "\n",
    "Our first model will be the very simple k-Nearest Neighbors model (KNN). KNN predicts an outcome for a given review based on the most similar reviews in the training data, making this a good way of establishing if reviews containing similar features such as length, rating, or sentiment tend to all receive similar levels of helpfulness. While intuitive and easy to implement, KNN suffers some drawbacks, such as sensitivity to feature scaling, and it slows with large datasets; it will primarily be used here as a point of comparison to other approaches. \n",
    "\n",
    "Then, we will apply Logistic Regression to construct a more robust yet more interpretable model. Logistic regression performs estimation of the probability that a given review is helpful and helps in identifying which features bear the strongest influence on that outcome. Its simplicity and interpretability are reasons it can serve well as a kind of benchmark for understanding the key factors behind review helpfulness. \n",
    "\n",
    "We will also be employing Principal Component Analysis in order to reduce the dimensionality of the data, particularly for text-based and numeric features. This will keep most of the important variation information in a smaller number of components, thus helping models run more efficiently and limiting the possibility of overfitting. \n",
    "\n",
    "Finally, we will test a small Neural Network to grasp more complex relationships between variables. Neural networks can model the kind of pattern that might elude simpler models, especially in text features like review summaries. Since our data is imbalanced and pretty big, we have chosen a lightweight structure in order to keep the training stable and the results interpretable. \n",
    "\n",
    "In general, this combination of models will enable us to make a comparison of various modeling approaches in view of predicting which reviews are likely to receive more than five helpful votes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9c3c0",
   "metadata": {},
   "source": [
    "Model Training Procedure - Morgan Simmons\n",
    "\n",
    "The training process will begin by dividing the data into training and testing sets, ensuring that the model is evaluated on reviews it has not seen before. A time-based split will be used so that the model trains on earlier reviews and tests on later ones, reflecting a realistic prediction scenario in which future data is unknown at training time. \n",
    "\n",
    "All numerical variables, including overall rating and review length, will be standardized to ensure that no single feature dominates the distance-based methods such as KNN. Text data from the summary field will be transformed into numerical features using a TF-IDF vectorization approach, which represents each wordâ€™s importance relative to its frequency across all reviews. This allows the model to capture key linguistic patterns that may be associated with higher helpfulness scores. \n",
    "\n",
    "After preprocessing, each model will be trained separately using the same training data to allow for direct comparison. For KNN, we will experiment with different values of k to identify the number of neighbors that yields the highest validation accuracy. For logistic regression, the regularization strength will be tuned to avoid overfitting while maintaining interpretability. When incorporating PCA, the number of principal components retained will be determined by the proportion of variance explained, typically around 90 to 95 percent. For the neural network, we will train a small feedforward model with one hidden layer and apply techniques such as early stopping to prevent overfitting. \n",
    "\n",
    "Throughout the training process, cross-validation will be used to assess model performance and stability. Since the dataset is imbalanced, class weighting will be applied so that the minority class, representing helpful reviews, has a stronger influence during training. Model performance will be evaluated primarily using accuracy, precision, recall, F1-score, and ROC-AUC, which together provide a balanced view of predictive ability. \n",
    "\n",
    "The combination of careful preprocessing, cross-validation, and class balancing will ensure that the final model is both fair and generalizable, providing insight into the factors that make a review more likely to be deemed helpful at the time it is posted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859c459",
   "metadata": {},
   "source": [
    "Model Validation Plan - Mark Haller\n",
    "\n",
    "Building on the training procedure above, we validate the helpful (>5 votes) classifier with a chronological split to mirror deployment and avoid look-ahead bias. Reviews are ordered by timestamp and divided into Train (oldest ~70%), Validation (next ~15%), and Test (most recent ~15%); the test window remains locked until final scoring. All preprocessing such as imputation, scaling, TF-IDF, and optional TruncatedSVD is wrapped in a single scikit-learn pipeline fit only on the current training fold to prevent leakage. Within the training slice, we tune hyperparameters using a rolling TimeSeriesSplit, so every validation fold uses later data than its training portion. Because the positive class is minority, our primary metric is PR-AUC; we also report Brier score with a reliability curve to assess calibration. The classification threshold is chosen only on the validation window by maximizing F1 and then frozen before scoring the test set. To address imbalance during training, we use class_weight='balanced' and display the baseline precision (prevalence) alongside PR curves for context. Final test reporting includes PR-AUC, ROC-AUC, Brier score, the confusion matrix at the frozen threshold, and 95% confidence intervals via CV fold variability (or a blocked bootstrap by week if time permits). We also include brief drift checks  and a short error analysis of high-confidence false positives/negatives to ensure the chosen operating point is stable and interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4fa10b",
   "metadata": {},
   "source": [
    "Other Deatils About Model Implementation - Rachel Kang\n",
    "\n",
    "All models will be through Python. All results and codes will be in our codebook.ipynb presented in our machine_learning_project repository. As mentioned before KNN and logistic regression will be used for the model. Additionally, we will be implementing class weighting to make sure that results are not biased and that we can get the most accurate results for this project. All codes will be run through Github and through ipynb files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e69fa2",
   "metadata": {},
   "source": [
    "Citation: \n",
    "\n",
    "OpenAI. ChatGPT, Oct 22 2025 version, OpenAI, https://chat.openai.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
